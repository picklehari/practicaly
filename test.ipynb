{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Model and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import fetch_data\n",
    "from dotenv import dotenv_values\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import MeanShift\n",
    "import numpy\n",
    "from markdown_pdf import MarkdownPdf,Section\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "variables = dotenv_values(\".env\")\n",
    "gen_model = \"llama3-8b-8192\"\n",
    "embedding_model = \"nomic-embed-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = \"https://wow.groq.com/retrieval-augmented-generation-with-groq-api/\"\n",
    "content_type = \"url\"\n",
    "\n",
    "client = Groq(api_key=variables[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_prompt = '''\n",
    "Given the following excrepts compiled from textbooks and lecture transcripts on a subject.\n",
    "\n",
    "{content}\n",
    "\n",
    "Identify core topics discussed and provide them an importance score.\n",
    "'''\n",
    "\n",
    "content_prompt = '''\n",
    "Given the following excrepts compiled from textbooks and lecture transcripts on a subject.\n",
    "\n",
    "{content}\n",
    "\n",
    "Clean the contents and make a comprehensive lecture notes on the topics being covered. Stick to the contents\n",
    "'''\n",
    "\n",
    "question_prompt = ''' \n",
    "Given the following lecture notes.\n",
    "\n",
    "<lecture_notes>\n",
    "{lecture_notes}\n",
    "</lecture_notes>\n",
    "\n",
    "Topic importance of each topic discussed in the lecture is given below.\n",
    "\n",
    "<topic importance>\n",
    "{topic_imp}\n",
    "<topic importance>\n",
    "You are a Teacher tasked with setting up a large number of questions for an upcoming examination. The number of questions per topic should depend upon the topic importance.\n",
    "The questions should include conceptual, reasoning and application level questions. Do not generate answers. Generate questions and not a question distribution\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "   return ollama.embeddings(model=embedding_model, prompt=text)['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllabus(content_clusters):\n",
    "  labels = set(content_clusters.values())\n",
    "  syllabus_list = []\n",
    "  for label in tqdm(labels):\n",
    "    content = \"\\n\".join([ct for ct,lb in content_clusters.items() if lb == label])\n",
    "\n",
    "    topic_response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful professor tasked with teaching and testing knowledge of students.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": topic_prompt.replace(\"{content}\",content),\n",
    "        }\n",
    "    ],\n",
    "    model=gen_model\n",
    ").choices[0].message.content\n",
    "    content_response =  client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful professor tasked with teaching and testing knowledge of students.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_prompt.replace(\"{content}\",content),\n",
    "        }\n",
    "    ],\n",
    "    model=gen_model\n",
    ").choices[0].message.content\n",
    "    \n",
    "    syllabus_list.append((topic_response,content_response))\n",
    "    time.sleep(7)\n",
    "\n",
    "    \n",
    "  return syllabus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(content_tuple):\n",
    "    content_dict ={\"Lecture Note\":content_tuple[1],\"Topic Importance\": content_tuple[0]}\n",
    "    question_content = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful professor tasked with teaching and testing knowledge of students.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question_prompt.replace(\"{lecture_notes}\",content_tuple[1]).replace(\"{topic_imp}\",content_tuple[0]),\n",
    "        }\n",
    "    ],\n",
    "    model=gen_model\n",
    ").choices[0].message.content\n",
    "    content_dict[\"Question Paper\"] = question_content\n",
    "    time.sleep(30)\n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7145.32it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.24s/it]\n"
     ]
    }
   ],
   "source": [
    "content_text = fetch_data.fetch_input(content,content_type)\n",
    "content_text = [ct for ct in tqdm(content_text) if ct.replace(\"\\n\",\"\").replace(\" \",\"\") != \"\"]\n",
    "content_embedding = [get_embedding(ct) for ct in tqdm(content_text)]\n",
    "content_embedding = numpy.array(content_embedding)\n",
    "clusters = MeanShift().fit(content_embedding)\n",
    "content_clusters = dict(zip(content_text,clusters.labels_))\n",
    "content_model = syllabus(content_clusters)\n",
    "content_dict = [generate_questions(ct) for ct in content_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chapters(content_dict:dict, out_path:str) -> str:\n",
    "    out_pdf = MarkdownPdf()\n",
    "    out_content = \"\"  # Initialize out_content variable\n",
    "    out_pdf.add_section(Section(\"# \" + content.split(\"/\")[-1].split(\".\")[0] + \"\\n\"))\n",
    "    for cd in content_dict:\n",
    "        out_content += \"## Section 01\\n\"\n",
    "        out_content += \"### Topics Discussed\\n\"\n",
    "        out_content += cd[\"Topic Importance\"] + \"\\n\"\n",
    "        out_content += \"### Notes\\n\"\n",
    "        out_content += cd[\"Lecture Note\"] + \"\\n\"\n",
    "        out_content += \"### Sample Questions\\n\"\n",
    "        out_content += cd[\"Question Paper\"] + \"\\n\\n\"\n",
    "    out_pdf.add_section(Section(out_content))\n",
    "    out_pdf.save(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_chapters(content_dict,\"notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
