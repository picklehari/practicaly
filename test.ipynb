{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Model and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import fetch_data\n",
    "from dotenv import dotenv_values\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import MeanShift\n",
    "import numpy\n",
    "from markdown_pdf import MarkdownPdf,Section\n",
    "\n",
    "import ollama\n",
    "import time\n",
    "variables = dotenv_values(\".env\")\n",
    "gen_model = \"llama3-8b-8192\"\n",
    "embedding_model = \"nomic-embed-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = \"https://wow.groq.com/retrieval-augmented-generation-with-groq-api\"\n",
    "content_type = \"url\"\n",
    "\n",
    "client = Groq(api_key=variables[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_prompt = '''\n",
    "Given the following excrepts compiled from textbooks and lecture transcripts on a subject.\n",
    "\n",
    "{content}\n",
    "\n",
    "Identify core topics discussed and provide them an importance score.\n",
    "'''\n",
    "\n",
    "content_prompt = '''\n",
    "Given the following excrepts compiled from textbooks and lecture transcripts on a subject.\n",
    "\n",
    "{content}\n",
    "\n",
    "Clean the contents and make a comprehensive lecture notes on the topics being covered. Stick to the contents\n",
    "'''\n",
    "\n",
    "question_prompt = ''' \n",
    "Given the following lecture notes.\n",
    "\n",
    "<lecture_notes>\n",
    "{lecture_notes}\n",
    "</lecture_notes>\n",
    "\n",
    "Topic importance of each topic discussed in the lecture is given below.\n",
    "\n",
    "<topic importance>\n",
    "{topic_imp}\n",
    "<topic importance>\n",
    "You are a Teacher tasked with setting up a large number of questions for an upcoming examination. The number of questions per topic should depend upon the topic importance.\n",
    "The questions should include conceptual, reasoning and application level questions. Do not generate answers. Generate questions and not a question distribution\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "   return ollama.embeddings(model=embedding_model, prompt=text)['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllabus(content_clusters):\n",
    "  labels = set(content_clusters.values())\n",
    "  syllabus_list = []\n",
    "  for label in tqdm(labels):\n",
    "    content = \"\\n\".join([ct for ct,lb in content_clusters.items() if lb == label])\n",
    "\n",
    "    topic_response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful professor tasked with teaching and testing knowledge of students.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": topic_prompt.replace(\"{content}\",content),\n",
    "        }\n",
    "    ],\n",
    "    model=gen_model\n",
    ").choices[0].message.content\n",
    "    content_response =  client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful professor tasked with teaching and testing knowledge of students.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_prompt.replace(\"{content}\",content),\n",
    "        }\n",
    "    ],\n",
    "    model=gen_model\n",
    ").choices[0].message.content\n",
    "    \n",
    "    syllabus_list.append((topic_response,content_response))\n",
    "    time.sleep(7)\n",
    "\n",
    "    \n",
    "  return syllabus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(content_tuple):\n",
    "    content_dict ={\"Lecture Note\":content_tuple[1],\"Topic Importance\": content_tuple[0]}\n",
    "    question_content = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful professor tasked with teaching and testing knowledge of students.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question_prompt.replace(\"{lecture_notes}\",content_tuple[1]).replace(\"{topic_imp}\",content_tuple[0]),\n",
    "        }\n",
    "    ],\n",
    "    model=gen_model\n",
    ").choices[0].message.content\n",
    "    content_dict[\"Question Paper\"] = question_content\n",
    "    time.sleep(30)\n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text = fetch_data.fetch_input(content,content_type)\n",
    "content_text = [ct for ct in tqdm(content_text) if ct.replace(\"\\n\",\"\").replace(\" \",\"\") != \"\"]\n",
    "content_embedding = [get_embedding(ct) for ct in tqdm(content_text)]\n",
    "content_embedding = numpy.array(content_embedding)\n",
    "clusters = MeanShift().fit(content_embedding)\n",
    "content_clusters = dict(zip(content_text,clusters.labels_))\n",
    "content_model = syllabus(content_clusters)\n",
    "content_dict = [generate_questions(ct) for ct in content_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = [{'Lecture Note': \"Lecture Notes: Retrieval Augmented Generation with Groq API\\n\\nI. Introduction\\n\\n* The emergence of Large Language Models (LLMs) has transformed the way we interact with information\\n* LLMs come with limitations, such as:\\n\\t+ Dated models and information\\n\\t+ Absence of domain-specific knowledge\\n\\t+ Inaccurate but plausible answers\\n* Enter Retrieval Augmented Generation (RAG), an approach that addresses these limitations\\n\\nII. What is Retrieval Augmented Generation (RAG)?\\n\\n* RAG combines the strengths of information retrieval methods and LLMs\\n* It harnesses pre-existing knowledge through a retrieval mechanism, allowing the model to pull in relevant information from a vast repository of data\\n* This ensures that the generated content is not only contextually accurate but also grounded in real-world information\\n* RAG aims to bridge the gap between traditional LLMs and human-like understanding\\n\\nIII. How does RAG help reduce the limitations of LLMs?\\n\\n* Dated Models and Information: RAG ensures the responsiveness of LLMs by consistently aligning generated responses with the latest, precise information sourced from an external database\\n* Absence of domain-specific knowledge: RAG overcomes this hurdle by enriching the model's context with domain-specific data from an organization's knowledge base\\n* Inaccurate but plausible answers: RAG combines generative capabilities with information retrieval, leveraging external knowledge to enhance the accuracy, contextuality, and reliability of the generated responses\\n\\nIV. Integrating RAG with Groq API\\n\\n* Connecting proprietary data to the Groq API is straightforward\\n* Steps:\\n\\t1. Connect to your database\\n\\t2. Convert questions into a vector representation using an embedding model\\n\\t3. Query your database\\n\\t4. Add the retrieved information to the LLM system prompt\\n\\t5. Ask Groq API to answer your question\\n\\nV. Public Sector Applications\\n\\n* Despite unique challenges, leveraging LLMs remains feasible in the Public Sector\\n* RAG can be a strategic approach for anchoring LLMs in the most current and verifiable information\\n* RAG contributes to building user trust in the system, a crucial element in the Public Sector where transparency and precision are paramount\\n\\nVI. Potential Examples of how Public Sector Organizations can leverage LLMs with RAG\\n\\n* Customers can optimize their utilization of proprietary data in conjunction with open source LLMs running on the Groq hardware to extract the full power of LLMs\\n* Customization is possible using own set of documents, other Vector Databases, other embedding models, and text generation LLMs available on Groq API\",\n",
    "  'Topic Importance': \"Based on the provided excerpts, I've identified core topics and provided an importance score:\\n\\n1. **Overview of Retrieval Augmented Generation (RAG)** (Importance: 9/10)\\n\\t* Definition of RAG and its purpose\\n\\t* Overview of how RAG combines information retrieval and LLMs to enhance contextual understanding and content accuracy\\n2. **Limitations of Large Language Models (LLMs)** and how RAG addresses them (Importance: 8.5/10)\\n\\t* Discussion of limitations, such as outdated models, lack of domain-specific knowledge, and inaccurate but plausible answers\\n\\t* How RAG mitigates these limitations to improve the reliability and utility of LLMs\\n3. **Integrating RAG with Groq API** (Importance: 8.5/10)\\n\\t* Step-by-step instructions on how to connect proprietary data to the Groq API using Python\\n\\t* Overview of how to use RAG to enhance the accuracy and contextuality of generated responses\\n4. **Benefits of RAG in Public Sector Organizations** (Importance: 8/10)\\n\\t* Discussion of how RAG can enhance the reliability and accuracy of responses in public sector organizations\\n\\t* Overview of the benefits of using RAG to build user trust and confidence in the system\\n5. **Potential Examples of Public Sector Organizations leveraging LLMs with RAG** (Importance: 7.5/10)\\n\\t* Overview of how public sector organizations can leverage LLMs with RAG to extract the full power of LLMs\\n\\t* Examples of customization and usage of RAG with LLMs\\n\\nNote that the importance scores are subjective and based on my interpretation of the relevance and significance of each topic within the larger context of the text.\",\n",
    "  'Question Paper': \"Based on the topic importance, I've created a set of questions for each topic. Since topic importance scores vary, I've allocated more questions to the more important topics.\\n\\n**Overview of Retrieval Augmented Generation (RAG)** (9/10)\\n\\n1. What is Retrieval Augmented Generation (RAG), and what problem does it solve in the context of Large Language Models (LLMs)?\\n2. How does RAG combine information retrieval and LLMs to enhance contextual understanding and content accuracy?\\n3. What are the primary goals of RAG, and how does it address the limitations of LLMs?\\n\\n**Limitations of Large Language Models (LLMs)** and how RAG addresses them (8.5/10)\\n\\n4. What are some of the limitations of Large Language Models (LLMs)?\\n5. How do outdated models and information pose a challenge for LLMs, and how does RAG address this limitation?\\n6. What is the role of domain-specific knowledge in LLMs, and how does RAG overcome the absence of such knowledge?\\n\\n**Integrating RAG with Groq API** (8.5/10)\\n\\n7. How do you connect proprietary data to the Groq API using Python, and what benefits does this integration provide?\\n8. What are the key steps involved in using RAG with the Groq API, and how does this process enhance the accuracy and contextuality of generated responses?\\n\\n**Benefits of RAG in Public Sector Organizations** (8/10)\\n\\n9. How can RAG enhance the reliability and accuracy of responses in public sector organizations, and what benefits does this provide?\\n10. What role does trust play in public sector applications, and how does RAG contribute to building user confidence in the system?\\n\\n**Potential Examples of Public Sector Organizations leveraging LLMs with RAG** (7.5/10)\\n\\n11. How can public sector organizations leverage LLMs with RAG to extract the full power of LLMs, and what benefits does this provide?\\n12. What are some potential customization options for public sector organizations using RAG with LLMs, and how can they be implemented?\\n\\nAdditional questions:\\n\\n13. How does RAG ensure the relevance and accuracy of generated responses, and what role does information retrieval play in this process?\\n14. What are some potential applications of RAG beyond public sector organizations, and how might it be used in other domains?\\n15. How does RAG compare to other approaches to enhancing the accuracy and contextuality of LLMs, and what advantages does it offer?\\n\\nPlease note that these questions are meant to be a starting point and may require further refinement or modification to better align with the specific needs and goals of your examination.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chapters(content_dict:dict, out_path:str) -> str:\n",
    "    out_pdf = MarkdownPdf()\n",
    "    out_content = \"\"  # Initialize out_content variable\n",
    "    out_pdf.add_section(Section(\"# \" + content.split(\"/\")[-1].split(\".\")[0] + \"\\n\"))\n",
    "    for cd in content_dict:\n",
    "        out_content += \"## Section 01\\n\"\n",
    "        out_content += \"### Topics Discussed\\n\"\n",
    "        out_content += cd[\"Topic Importance\"] + \"\\n\"\n",
    "        out_content += \"### Notes\\n\"\n",
    "        out_content += cd[\"Lecture Note\"] + \"\\n\"\n",
    "        out_content += \"### Sample Questions\\n\"\n",
    "        out_content += cd[\"Question Paper\"] + \"\\n\\n\"\n",
    "    out_pdf.add_section(Section(out_content))\n",
    "    print(cd)\n",
    "    out_pdf.save(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_chapters(content_dict,\"notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data.download_audio(\"https://youtu.be/om7TfE7cUko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTube(\"https://youtu.be/om7TfE7cUko\").streams.filter(type=\"video\").all()[0].download(\"Data/temp.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg.input(\"/Users/picklehari/Desktop/Code/practicaly_llm/Data/Life Lesson from Smokers  Take 01.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
